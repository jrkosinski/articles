{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3800cf3-7d7a-4a51-a37a-15b7e802a9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import keras\n",
    "import math \n",
    "import random \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lib.Trend import TrendData, extract_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedee0d-0373-4369-8dfd-05aae514adf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ef7a9b9-5423-4638-8e29-0168aa2ff41a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Range</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.989125</td>\n",
       "      <td>0.353968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Range    Change\n",
       "0  1.000000  0.437608\n",
       "1  1.000000  0.057881\n",
       "2  1.000000  0.000000\n",
       "3  1.000000  0.000000\n",
       "4  0.989125  0.353968"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "def scale_col_values(\n",
    "    df: pd.DataFrame, \n",
    "    col_name:str, \n",
    "    min_value:float=0, \n",
    "    max_value:float=1\n",
    "): \n",
    "    values = df[col_name].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(min_value, max_value))\n",
    "    scaled_values = scaler.fit_transform(values)\n",
    "    df[col_name] = scaled_values.transpose()[0]\n",
    "    return df\n",
    "\n",
    "def squash_col_outliers(\n",
    "    df: pd.DataFrame, \n",
    "    col_name: str, \n",
    "    min_quantile: float =0.01, \n",
    "    max_quantile:float =0.99\n",
    "): \n",
    "    q_lo = df[col_name].quantile(min_quantile)\n",
    "    q_hi  = df[col_name].quantile(max_quantile)\n",
    "    \n",
    "    df.loc[df[col_name] >= q_hi, col_name] = q_hi\n",
    "    df.loc[df[col_name] <= q_lo, col_name] = q_lo\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"data/prices-d.csv\", index_col=0)\n",
    "df.pop(\"Volume\")\n",
    "df[\"Range\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Open\"]\n",
    "df.pop(\"Open\")\n",
    "df.pop(\"High\")\n",
    "df.pop(\"Low\")\n",
    "df['Change'] = df[\"Adj Close\"].pct_change()\n",
    "df = df. tail(-1) \n",
    "df.pop(\"Close\")\n",
    "df = pd.DataFrame(df.values, columns=['Adj Close', 'Range', 'Change'])\n",
    "df = squash_col_outliers(df, 'Change')\n",
    "df = squash_col_outliers(df, \"Range\", min_quantile=0.0, max_quantile=0.97)\n",
    "df = scale_col_values(df, 'Change')\n",
    "df = scale_col_values(df, 'Range')\n",
    "\n",
    "trend = extract_trend(df['Adj Close'], 100)\n",
    "#df['Trend'] = trend.as_boolean(df['Adj Close'][0])\n",
    "df.pop(\"Adj Close\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cce00098-c4d8-404a-8854-53dc8a434aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFinally, we have 3 columns (or features): Range, Change, and Trend. \\nLet's pretend that Change is what we want the model to predict. \\n\\nThe input for a keras LSTM requires a three dimensional array with the shape: \\n(s, t, f) \\n\\ns = samples: the number of samples in the data set (i.e. the number of rows of data) \\nt = timesteps: the number of timesteps to be input for each sample (also sometimes called the 'lag')\\nf = features: the number of distinct features to be considered; in this case, 3 (Range, Change, Trend)\\n\\nAn LSTM can predict multiple output features, and can do so with a variable offset and width. But just to \\nkeep things simple, we'll assume for this example that the output offset is 1, the LSTM will predict only \\none output feature (Change), and it will predict for only one timestep: the next day's Change. \\n\\nNote also that the output feature need not be one of the input features as well. In this case, Change is \\npresent in both the input and the output. \\n\\nX represents the input values. \\ny represents the predicted or expected values. \\n\\nX: Range(t[-10:0]), Change(t[-10:0]), Trend(t[-10:0])\\ny: Change (t+1)\\n\\nSteps:\\n1. Extract the 'y' values, or the values to be predicted. This is supervised learning, so these are all \\nthe 'correct' answers for training. \\n\\n2. Window the appropriate number of timesteps for each input \\n\\n3. Add one example of each feature, to each window \\n\\nBecause the LSTM keeps a memory of more recent inputs, data is fed into it in a forward walking window the size\\nof a predetermined number of timesteps. Each discrete input contains multiple overlapping windows, and each \\nwindow contains one example of each feature. It's easier to explain with an example: \\n\\nThe raw input data has 10 rows of 2 features each: f1, f2. It looks like this: \\n\\n       f1   f2   \\nrow 0: r0f1 r0f2  \\nrow 1: r1f1 r1f2  \\nrow 2: r2f1 r2f2  \\nrow 3: r3f1 r3f2  \\nrow 4: r4f1 r4f2  \\nrow 5: r5f1 r5f2  \\n...\\nrow 9: r9f1 r9f2 r9f3\\n\\nSo the outermost dimension of the 3-dimensional input array will have 10 elements. Each of those elements \\nwill be an array, so let's create this to begin with: \\n\\n[ [] [] [] [] [] [] [] [] [] [] ]\\n\\nIt's an array containing 10 empty arrays. \\n\\nHow many timesteps? Let's say 3. So each of those empty arrays will have 3 arrays inside of them. Each of \\nthose innermost arrays will contain the 2 features. \\n\\nTo simplify, first create an array of 3-element arrays, where each element of the inner array represents \\none row. Since this is daily data, we'll call row 0 d0, row 1 is d1, and so on. \\n\\n[\\n  [      d0]\\n  [   d0 d1]\\n  [d0 d1 d2]\\n  [d1 d2 d3]\\n  [d2 d3 d4]\\n  [d4 d5 d6]\\n  [d5 d6 d7]\\n  [d6 d7 d8]\\n  [d7 d8 d9]\\n]\\n\\nReplace each day (row) with an array containing the two features of that day (row). So d0 becomes \\nthe two-element array [r0f1, r0f2]. \\n\\n[\\n  [null        null        [r0f1 r0f2]]\\n  [null        [r0f1 r0f2] [r1f1 r1f2]]\\n  [[r0f1 r0f2] [r1f1 r1f2] [r2f1 r2f2]]\\n  [[r1f1 r1f2] [r2f1 r2f2] [r3f1 r3f2]]\\n  [[r2f1 r2f2] [r3f1 r3f2] [r4f1 r4f2]]\\n  [[r3f1 r3f2] [r4f1 r4f2] [r5f1 r5f2]]\\n  [[r4f1 r4f2] [r5f1 r5f2] [r6f1 r6f2]]\\n  [[r5f1 r5f2] [r6f1 r6f2] [r7f1 r7f2]]\\n  [[r6f1 r6f2] [r7f1 r7f2] [r8f1 r8f2]]\\n  [[r7f1 r7f2] [r8f1 r8f2] [r9f1 r9f2]]\\n]\\n\\nThe row numbers are ordinal in each column going from top to bottom, and ordinal from left to right. \\nThat's the input format. Since the first two rows contain nulls, we'd remove them. So we end up with \\nthe number of rows being r = (r - (timesteps - 1))\\n\\nNow the y values are just a scalar array of feature 2 from each row, but shift forward 1. \\n\\nX VALUES                                   Y VALUES\\n[                                        [\\n  [[r0f1 r0f2] [r1f1 r1f2] [r2f1 r2f2]]   r3f2\\n  [[r1f1 r1f2] [r2f1 r2f2] [r3f1 r3f2]]   r4f2\\n  [[r2f1 r2f2] [r3f1 r3f2] [r4f1 r4f2]]   r5f2\\n  [[r3f1 r3f2] [r4f1 r4f2] [r5f1 r5f2]]   r6f2\\n  [[r4f1 r4f2] [r5f1 r5f2] [r6f1 r6f2]]   r7f2\\n  [[r5f1 r5f2] [r6f1 r6f2] [r7f1 r7f2]]   r8f2\\n  [[r6f1 r6f2] [r7f1 r7f2] [r8f1 r8f2]]   r9f2\\n  [[r7f1 r7f2] [r8f1 r8f2] [r9f1 r9f2]]   ?\\n]                                        ]\\n\\nBecause the y values are shifted by one, we have to lose one more row from the training data - we don't \\nhave tomorrow's value, so we need to remove the last row this time. \\n\\nAnd that's the input shape for a tensorflow LSTM. \\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VI: Shaping the Data for LSTM Input\n",
    "\n",
    "'''\n",
    "Finally, we have 3 columns (or features): Range, Change, and Trend. \n",
    "Let's pretend that Change is what we want the model to predict. \n",
    "\n",
    "The input for a keras LSTM requires a three dimensional array with the shape: \n",
    "(s, t, f) \n",
    "\n",
    "s = samples: the number of samples in the data set (i.e. the number of rows of data) \n",
    "t = timesteps: the number of timesteps to be input for each sample (also sometimes called the 'lag')\n",
    "f = features: the number of distinct features to be considered; in this case, 3 (Range, Change, Trend)\n",
    "\n",
    "An LSTM can predict multiple output features, and can do so with a variable offset and width. But just to \n",
    "keep things simple, we'll assume for this example that the output offset is 1, the LSTM will predict only \n",
    "one output feature (Change), and it will predict for only one timestep: the next day's Change. \n",
    "\n",
    "Note also that the output feature need not be one of the input features as well. In this case, Change is \n",
    "present in both the input and the output. \n",
    "\n",
    "X represents the input values. \n",
    "y represents the predicted or expected values. \n",
    "\n",
    "X: Range(t[-10:0]), Change(t[-10:0]), Trend(t[-10:0])\n",
    "y: Change (t+1)\n",
    "\n",
    "Steps:\n",
    "1. Extract the 'y' values, or the values to be predicted. This is supervised learning, so these are all \n",
    "the 'correct' answers for training. \n",
    "\n",
    "2. Window the appropriate number of timesteps for each input \n",
    "\n",
    "3. Add one example of each feature, to each window \n",
    "\n",
    "Because the LSTM keeps a memory of more recent inputs, data is fed into it in a forward walking window the size\n",
    "of a predetermined number of timesteps. Each discrete input contains multiple overlapping windows, and each \n",
    "window contains one example of each feature. It's easier to explain with an example: \n",
    "\n",
    "The raw input data has 10 rows of 2 features each: f1, f2. It looks like this: \n",
    "\n",
    "       f1   f2   \n",
    "row 0: r0f1 r0f2  \n",
    "row 1: r1f1 r1f2  \n",
    "row 2: r2f1 r2f2  \n",
    "row 3: r3f1 r3f2  \n",
    "row 4: r4f1 r4f2  \n",
    "row 5: r5f1 r5f2  \n",
    "...\n",
    "row 9: r9f1 r9f2 r9f3\n",
    "\n",
    "So the outermost dimension of the 3-dimensional input array will have 10 elements. Each of those elements \n",
    "will be an array, so let's create this to begin with: \n",
    "\n",
    "[ [] [] [] [] [] [] [] [] [] [] ]\n",
    "\n",
    "It's an array containing 10 empty arrays. \n",
    "\n",
    "How many timesteps? Let's say 3. So each of those empty arrays will have 3 arrays inside of them. Each of \n",
    "those innermost arrays will contain the 2 features. \n",
    "\n",
    "To simplify, first create an array of 3-element arrays, where each element of the inner array represents \n",
    "one row. Since this is daily data, we'll call row 0 d0, row 1 is d1, and so on. \n",
    "\n",
    "[\n",
    "  [      d0]\n",
    "  [   d0 d1]\n",
    "  [d0 d1 d2]\n",
    "  [d1 d2 d3]\n",
    "  [d2 d3 d4]\n",
    "  [d4 d5 d6]\n",
    "  [d5 d6 d7]\n",
    "  [d6 d7 d8]\n",
    "  [d7 d8 d9]\n",
    "]\n",
    "\n",
    "Replace each day (row) with an array containing the two features of that day (row). So d0 becomes \n",
    "the two-element array [r0f1, r0f2]. \n",
    "\n",
    "[\n",
    "  [null        null        [r0f1 r0f2]]\n",
    "  [null        [r0f1 r0f2] [r1f1 r1f2]]\n",
    "  [[r0f1 r0f2] [r1f1 r1f2] [r2f1 r2f2]]\n",
    "  [[r1f1 r1f2] [r2f1 r2f2] [r3f1 r3f2]]\n",
    "  [[r2f1 r2f2] [r3f1 r3f2] [r4f1 r4f2]]\n",
    "  [[r3f1 r3f2] [r4f1 r4f2] [r5f1 r5f2]]\n",
    "  [[r4f1 r4f2] [r5f1 r5f2] [r6f1 r6f2]]\n",
    "  [[r5f1 r5f2] [r6f1 r6f2] [r7f1 r7f2]]\n",
    "  [[r6f1 r6f2] [r7f1 r7f2] [r8f1 r8f2]]\n",
    "  [[r7f1 r7f2] [r8f1 r8f2] [r9f1 r9f2]]\n",
    "]\n",
    "\n",
    "The row numbers are ordinal in each column going from top to bottom, and ordinal from left to right. \n",
    "That's the input format. Since the first two rows contain nulls, we'd remove them. So we end up with \n",
    "the number of rows being r = (r - (timesteps - 1))\n",
    "\n",
    "Now the y values are just a scalar array of feature 2 from each row, but shift forward 1. \n",
    "\n",
    "X VALUES                                   Y VALUES\n",
    "[                                        [\n",
    "  [[r0f1 r0f2] [r1f1 r1f2] [r2f1 r2f2]]   r3f2\n",
    "  [[r1f1 r1f2] [r2f1 r2f2] [r3f1 r3f2]]   r4f2\n",
    "  [[r2f1 r2f2] [r3f1 r3f2] [r4f1 r4f2]]   r5f2\n",
    "  [[r3f1 r3f2] [r4f1 r4f2] [r5f1 r5f2]]   r6f2\n",
    "  [[r4f1 r4f2] [r5f1 r5f2] [r6f1 r6f2]]   r7f2\n",
    "  [[r5f1 r5f2] [r6f1 r6f2] [r7f1 r7f2]]   r8f2\n",
    "  [[r6f1 r6f2] [r7f1 r7f2] [r8f1 r8f2]]   r9f2\n",
    "  [[r7f1 r7f2] [r8f1 r8f2] [r9f1 r9f2]]   ?\n",
    "]                                        ]\n",
    "\n",
    "Because the y values are shifted by one, we have to lose one more row from the training data - we don't \n",
    "have tomorrow's value, so we need to remove the last row this time. \n",
    "\n",
    "And that's the input shape for a tensorflow LSTM. \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ee65dc-2dad-408f-81da-2d299f750bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract y column (the col to be predicted)\n",
    "# df: the DataFrame\n",
    "# col_name: the name of the column to be predicted \n",
    "# ntimesteps: number of timesteps\n",
    "#\n",
    "def extract_y(df: pd.DataFrame, col_name: str, ntimesteps: int): \n",
    "    #TODO: don't need to shift here \n",
    "    shifted = df.shift(1)\n",
    "    shifted = shifted.tail(-1) \n",
    "    shifted = shifted.tail(-ntimesteps)\n",
    "    return shifted[colname].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8451dd64-9baf-44aa-845b-9fd7a140c351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract X with the given number of timesteps\n",
    "# df: the DataFrame\n",
    "# ntimesteps: number of timesteps\n",
    "#\n",
    "def extract_X(df: pd.DataFrame, ntimesteps: int): \n",
    "    features = len(df.columns)\n",
    "    X = list()\n",
    "    \n",
    "    #offset for timesteps\n",
    "    offsets = list()\n",
    "    for i in range (ntimesteps, 0, -1): \n",
    "        offsets.append(df.shift(i))\n",
    "        \n",
    "    #combine timestep columns into rows \n",
    "    combined = pd.concat(offsets, axis=1)\n",
    "    combined = combined.tail(-ntimesteps) \n",
    "    combined.drop(combined.tail(1).index, inplace=True)\n",
    "    \n",
    "    #reshape each row (timesteps, features)\n",
    "    for i in range(len(combined)): \n",
    "        row = combined.iloc[i].to_numpy()\n",
    "        xrow = list()\n",
    "        for n in range(ntimesteps): \n",
    "            xrow.append(row[n*features:(n*features)+features])\n",
    "        X.append(xrow)\n",
    "    \n",
    "    #return as numpy array\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce2441b-b69e-4748-93a4-efbf4a19d573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 10, 2)\n",
      "(3200,)\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT X and y\n",
    "timesteps = 10\n",
    "X = extract_X(df, timesteps)\n",
    "y = extract_y(df, 'Change', timesteps)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0547708-0b15-4a82-93a1-a97180d29b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, X, y): \n",
    "        if X.ndim != 3: \n",
    "            raise Exception(\"Expected a 3-dimensional array for X\")\n",
    "        if y.ndim != 1: \n",
    "            raise Exception(\"Expected a 1-dimensional array for y\")\n",
    "        if len(X) != len(y): \n",
    "            raise Exception(\"Length of X and y must be the same\")\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    # pct% of the dataset will be split off and returned as a new DataSet\n",
    "    def split(self, pct:float): \n",
    "        count = int(self.size * pct)\n",
    "        new_dataset = DataSet(self.X[:count], self.y[:count])\n",
    "        self.X = self.X[:-count]\n",
    "        self.y = self.y[:-count]\n",
    "        return new_dataset\n",
    "        \n",
    "    @property\n",
    "    def size(self): \n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a06cfd-b0cb-4f66-9783-85c5c143f21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Finally, the entire preprocessed dataset is split into 3 parts: approximately 70% - 20% - 10% \n",
    "split for training, evaluation, and testing respectively.\n",
    "'''\n",
    "train = DataSet(X, y)\n",
    "val = train.split(0.3)\n",
    "test = val.split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f5fe474-0b50-4243-b83e-35e84b37e5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240\n",
      "672\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "print(train.size)\n",
    "print(val.size)\n",
    "print(test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888d355-38f6-4ff9-af75-d4dad2e59dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b34236-8f98-4e24-a6df-87355a46c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815b255-deac-4d63-8abc-d2be7e84ec51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
